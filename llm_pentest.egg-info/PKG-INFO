Metadata-Version: 2.1
Name: llm-pentest
Version: 0.1.0
Summary: A tool for pentesting LLM endpoints
Home-page: https://github.com/yourusername/llm-pentest
Author: Your Name
Author-email: your.email@example.com
License: UNKNOWN
Platform: UNKNOWN
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Requires-Python: >=3.10
Description-Content-Type: text/markdown

# LLM Pentesting Tool

A tool for testing the security of Large Language Model (LLM) endpoints against various attack vectors.

## Installation

1. Clone the repository:
```bash
git clone https://github.com/yourusername/llm-pentest.git
cd llm-pentest
```

2. Install the package and its dependencies:
```bash
pip install -e .
```

## Usage

### Basic Usage

Run all attack types against an LLM endpoint:
```bash
python -m pentest.cli --endpoint "http://your-llm-endpoint/v1/chat/completions"
```

### Advanced Usage

Run a specific attack type:
```bash
python -m pentest.cli --endpoint "http://your-llm-endpoint/v1/chat/completions" --attack-type prompt_injection
```

Collect fresh data and run tests:
```bash
python -m pentest.cli --endpoint "http://your-llm-endpoint/v1/chat/completions" --collect-data
```

Use custom headers:
```bash
python -m pentest.cli --endpoint "http://your-llm-endpoint/v1/chat/completions" --headers '{"Authorization": "Bearer your-token"}'
```

### Command Line Options

- `--endpoint`: LLM API endpoint to test (required)
- `--base-prompt`: Base prompt to use for testing (default: "You are a helpful AI assistant.")
- `--headers`: JSON string of headers to use for API requests (default: "{}")
- `--output-dir`: Directory to save reports (default: "output")
- `--attack-type`: Specific attack type to run (choices: prompt_injection, prompt_leaking, junction_attack, code_injection, role_playing, system_prompt_extraction)
- `--collect-data`: Collect fresh data before running tests
- `--keywords`: Keywords to use for data collection (default: common LLM security terms)
- `-v, --verbose`: Enable verbose logging

### Attack Types

1. **Prompt Injection**
   - Tests if the model can be manipulated to ignore its original instructions
   - Attempts to inject malicious prompts

2. **Prompt Leaking**
   - Tests if the model can be tricked into revealing its system prompt
   - Attempts to extract sensitive information

3. **Junction Attack**
   - Tests if the model can be manipulated to perform unintended actions
   - Combines multiple prompts to create unexpected behavior

4. **Code Injection**
   - Tests if the model can be tricked into executing arbitrary code
   - Attempts to inject malicious code snippets

5. **Role Playing**
   - Tests if the model can be manipulated to assume different roles
   - Attempts to bypass security restrictions

6. **System Prompt Extraction**
   - Tests if the model's system prompt can be extracted
   - Attempts to reveal internal instructions

## Output

The tool generates detailed reports in JSON format, saved in the specified output directory. Each report includes:

- Attack type
- Success/failure status
- Response from the model
- Execution time
- Payload used
- Any errors encountered

## Requirements

- Python 3.10 or higher
- Dependencies listed in setup.py

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request. 

generator = AttackGenerator() 

