import logging
import json
from abc import ABC, abstractmethod
from typing import List, Dict
import requests
from bs4 import BeautifulSoup
import arxiv
import praw
from github import Github
from datetime import datetime, timedelta
from .classifier import AttackTypeClassifier
import re
from collections import Counter

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class BaseScraper(ABC):
    @abstractmethod
    def collect_data(self, keywords: List[str] = None) -> List[Dict]:
        """Collect data from the source."""
        pass

class ArxivScraper(BaseScraper):
    def collect_data(self, keywords: List[str] = None) -> List[Dict]:
        """Collect papers from arXiv related to LLM security."""
        if keywords is None:
            keywords = ["LLM security", "language model security", "prompt injection"]
            
        results = []
        for keyword in keywords:
            try:
                search = arxiv.Search(
                    query=keyword,
                    max_results=10,
                    sort_by=arxiv.SortCriterion.SubmittedDate
                )
                
                for paper in search.results():
                    results.append({
                        "id": paper.entry_id,
                        "title": paper.title,
                        "description": paper.summary,
                        "source": "arxiv",
                        "url": paper.pdf_url,
                        "added_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    })
            except Exception as e:
                logging.error(f"Error fetching from arXiv: {str(e)}")
                
        return results

class GitHubScraper(BaseScraper):
    def collect_data(self, keywords: List[str] = None) -> List[Dict]:
        """Collect data from GitHub repositories."""
        if keywords is None:
            keywords = ["LLM security", "prompt injection", "LLM pentesting"]
            
        results = []
        try:
            g = Github()  # For unauthenticated requests
            for keyword in keywords:
                repos = g.search_repositories(
                    query=f"{keyword} in:readme,description",
                    sort="updated",
                    order="desc"
                )
                
                for repo in repos[:5]:  # Limit to top 5 repos per keyword
                    results.append({
                        "id": str(repo.id),
                        "title": repo.name,
                        "description": repo.description or "",
                        "source": "github",
                        "url": repo.html_url,
                        "added_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    })
        except Exception as e:
            logging.error(f"Error fetching from GitHub: {str(e)}")
            
        return results

class RedditScraper(BaseScraper):
    def collect_data(self, keywords: List[str] = None) -> List[Dict]:
        """Collect data from Reddit."""
        if keywords is None:
            keywords = ["LLM security", "prompt injection", "AI security"]
            
        results = []
        try:
            reddit = praw.Reddit(
                client_id="YOUR_CLIENT_ID",
                client_secret="YOUR_CLIENT_SECRET",
                user_agent="LLMSecurityMonitor/1.0"
            )
            
            for keyword in keywords:
                for submission in reddit.subreddit("MachineLearning+cybersecurity").search(
                    keyword, sort="new", time_filter="month"
                ):
                    results.append({
                        "id": submission.id,
                        "title": submission.title,
                        "description": submission.selftext,
                        "source": "reddit",
                        "url": f"https://reddit.com{submission.permalink}",
                        "added_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    })
        except Exception as e:
            logging.error(f"Error fetching from Reddit: {str(e)}")
            
        return results

class OWASPScraper(BaseScraper):
    def collect_data(self, keywords: List[str] = None) -> List[Dict]:
        """Collect vulnerability data from OWASP LLM Top 10."""
        results = []
        try:
            url = "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
            response = requests.get(url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            vulns = soup.find_all('h2')  # Assuming vulnerabilities are h2 headers
            
            for vuln in vulns:
                if vuln.text.strip():
                    results.append({
                        "id": f"owasp_{len(results)}",
                        "title": vuln.text.strip(),
                        "description": vuln.find_next('p').text if vuln.find_next('p') else "",
                        "source": "owasp",
                        "url": url,
                        "added_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    })
        except Exception as e:
            logging.error(f"Error fetching from OWASP: {str(e)}")
            
        return results

class MITREScraper:
    def collect_data(self) -> List[Dict]:
        """
        Collect technique data from MITRE ATT&CK.
        """
        try:
            logger.info("Collecting data from MITRE ATT&CK")
            url = "https://attack.mitre.org/techniques/enterprise/"
            response = requests.get(url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            results = []
            
            # Find all technique entries
            for entry in soup.find_all('tr', class_='technique'):
                name = entry.find('td', class_='name')
                description = entry.find('td', class_='description')
                
                if name and description:
                    results.append({
                        'title': name.text.strip(),
                        'content': description.text.strip(),
                        'source': 'mitre',
                        'url': url
                    })
            
            logger.info(f"Found {len(results)} techniques from MITRE")
            return results
            
        except Exception as e:
            logger.error(f"Error collecting data from MITRE: {str(e)}")
            return []

class SecurityBlogScraper:
    def __init__(self):
        self.blogs = [
            "https://www.bleepingcomputer.com/",
            "https://www.securityweek.com/",
            "https://www.darkreading.com/",
            "https://www.helpnetsecurity.com/"
        ]
        self.classifier = AttackTypeClassifier()
        self.keyword_patterns = {
            'prompt_injection': r'prompt\s+injection|prompt\s+manipulation|prompt\s+hacking',
            'code_injection': r'code\s+injection|arbitrary\s+code\s+execution|RCE',
            'data_leakage': r'data\s+leak|information\s+disclosure|sensitive\s+data',
            'model_extraction': r'model\s+extraction|model\s+stealing|model\s+theft',
            'adversarial': r'adversarial\s+attack|adversarial\s+example|adversarial\s+prompt'
        }
    
    def _extract_key_phrases(self, text: str) -> List[str]:
        """Extract key phrases from text using simple NLP techniques."""
        # Remove special characters and convert to lowercase
        text = re.sub(r'[^\w\s]', ' ', text.lower())
        
        # Split into words and remove stop words
        words = text.split()
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'was', 'were', 'in', 'on', 'at', 'to', 'for'}
        words = [word for word in words if word not in stop_words]
        
        # Find n-grams (phrases of 2-3 words)
        phrases = []
        for i in range(len(words) - 1):
            phrases.append(' '.join(words[i:i+2]))
        for i in range(len(words) - 2):
            phrases.append(' '.join(words[i:i+3]))
        
        return phrases
    
    def _analyze_text(self, text: str) -> Dict:
        """Analyze text using NLP techniques."""
        # Classify using the attack type classifier
        classification = self.classifier.classify_text(text)
        
        # Extract key phrases
        key_phrases = self._extract_key_phrases(text)
        
        # Count keyword occurrences
        keyword_counts = {}
        for category, pattern in self.keyword_patterns.items():
            matches = len(re.findall(pattern, text, re.IGNORECASE))
            if matches > 0:
                keyword_counts[category] = matches
        
        return {
            'classification': classification,
            'key_phrases': key_phrases,
            'keyword_counts': keyword_counts
        }
    
    def collect_data(self, keywords: List[str]) -> List[Dict]:
        """
        Collect and analyze articles from security blogs.
        
        Args:
            keywords: List of keywords to search for
        """
        try:
            logger.info(f"Collecting data from security blogs with keywords: {keywords}")
            results = []
            
            for blog_url in self.blogs:
                try:
                    response = requests.get(blog_url)
                    response.raise_for_status()
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    articles = soup.find_all('article')
                    
                    for article in articles:
                        title = article.find('h2') or article.find('h1')
                        content = article.find('div', class_='content') or article.find('p')
                        
                        if title and content:
                            text = f"{title.text.strip()} {content.text.strip()}"
                            
                            # Perform NLP analysis
                            analysis = self._analyze_text(text)
                            
                            # Check if article is relevant based on NLP analysis
                            is_relevant = (
                                any(keyword.lower() in text.lower() for keyword in keywords) or
                                any(count > 0 for count in analysis['keyword_counts'].values()) or
                                max(analysis['classification'].values()) > 0.5
                            )
                            
                            if is_relevant:
                                results.append({
                                    'title': title.text.strip(),
                                    'content': content.text.strip(),
                                    'source': blog_url,
                                    'url': article.find('a')['href'] if article.find('a') else blog_url,
                                    'analysis': analysis
                                })
                    
                except Exception as e:
                    logger.error(f"Error collecting data from {blog_url}: {str(e)}")
                    continue
            
            logger.info(f"Found {len(results)} relevant articles from security blogs")
            return results
            
        except Exception as e:
            logger.error(f"Error collecting data from security blogs: {str(e)}")
            return [] 